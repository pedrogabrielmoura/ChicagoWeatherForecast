{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c8a30a-5014-42f8-b0c1-193817c2ccff",
   "metadata": {},
   "source": [
    "# Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6727877-54ff-4b33-8daa-dace7e131f61",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "In the previous notebook, ts_model.ipynb, we managed to remove all time dependency, seasonality and autocorrelation, from our target feature, allowing us to transition to a classic machine learning approach, specifically, a tree-based model.\n",
    "\n",
    "In this notebook, we performed feature engineering, model selection, feature selection, and finally model tuning. The candidate models for this problem are LightGBM and XGBoost, due to their typically high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb6c70-f7a8-4f8b-9aba-e17e62fc0f0f",
   "metadata": {},
   "source": [
    "**Data Source**\n",
    "The data used in this notebook was extracted from the notebook *model/ts_model.ipynb*\n",
    "\n",
    "- **Data:** 06/12/2025\n",
    "- **Localização:** ../data/wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8397c-3c95-4a9d-98c4-75e679c7ab53",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7a357-7730-4a42-9510-0f21ecab6526",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19eeb0ac-5ec3-4430-a65d-1aa8c692d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"lightgbm.engine\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"lightgbm.basic\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1083c3c0-c686-4571-b78d-93c1cde9e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções criadas\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, Path.cwd().parents[1].as_posix())\n",
    "\n",
    "from src.ts_utils import *\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543217c0-0486-4e8b-8998-4696c4e01d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['#003366'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0640dd-4a68-4671-b1df-b8d860e47003",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our target variable contains 6 years of data, totaling 2,190 rows, due to the seasonal division applied in the previous notebook. This amount of information is typically small for training tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab5fc23-5909-4d83-8082-b2cdff78324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2555 entries, 0 to 2554\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   time      2555 non-null   datetime64[ns]\n",
      " 1   tavg      2555 non-null   float64       \n",
      " 2   prcp      2555 non-null   float64       \n",
      " 3   snow      2555 non-null   float64       \n",
      " 4   wspd      2555 non-null   float64       \n",
      " 5   pres      2555 non-null   float64       \n",
      " 6   tamp      2555 non-null   float64       \n",
      " 7   sin_1     2555 non-null   float64       \n",
      " 8   cos_1     2555 non-null   float64       \n",
      " 9   ml_resid  2555 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9)\n",
      "memory usage: 199.7 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(os.path.join(DATA_PATH_WRANGLE, 'weather_linear_resid.parquet'))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea752489-5b37-40b4-9f66-41751c5007a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2555.000000\n",
       "mean        0.002710\n",
       "std         3.179228\n",
       "min       -15.814890\n",
       "25%        -1.684089\n",
       "50%         0.196558\n",
       "75%         1.828278\n",
       "max        11.023475\n",
       "Name: ml_resid, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ml_resid'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117b479-9a01-48ba-9923-05354e29073a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "In this section, we create all the features our models might use. The plan is to generate as many features as possible to expose all the information contained in the data and then later decide which ones should remain in the final model. In other words, we are not concerned at this stage with whether a feature is useful, this will be determined later in the notebook.\n",
    "\n",
    "The only rule is that the feature must be available at the time of forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12caa6d5-2936-4259-b2d0-802193af4647",
   "metadata": {},
   "source": [
    "## Lagging\n",
    "\n",
    "Given this rule, we begin by lagging all relevant features to ensure they are available at forecast time. We applied lags from 365 to 730 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c05baf-95e0-4bff-9cb1-07765bf9b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['tavg', 'prcp', 'snow', 'wspd', 'pres', 'tamp']\n",
    "for c in list_columns:\n",
    "    for lag in range(365, 2*365):\n",
    "        df[f'{c}_{lag}'] = df[c].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6c3a85-b8e7-4d0e-adbe-8ce047189b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1826, 2200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05c872-7412-4da0-9ecd-75d9eb9b0a8e",
   "metadata": {},
   "source": [
    "So far, our table has 2.202 columns and 1.826 rows, about 5 years of data. After applying the lags, we will also include the differences, which capture the day-to-day changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7b478-f15c-4f2f-8c29-2f85a50d976f",
   "metadata": {},
   "source": [
    "## Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee97d9b1-0421-40f1-82b7-61eacba39913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_include = {'time', 'tavg', 'prcp', 'snow', 'wspd', 'pres', 'tamp', 'sin_1', 'cos_1', 'seasonal_resid', 'ml_resid', 'seasonal_pattern'}\n",
    "set_all_columns = set(df.columns)\n",
    "set_columns = set_all_columns - not_include\n",
    "\n",
    "len(set_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d3a655-2743-469b-9113-a67c5d95b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in set_columns:\n",
    "    df[f'{c}_diff'] = df[c].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8767e6a3-1fde-42e7-84a4-16d2f24909b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 4390)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd091e5a-0fa9-40d6-8643-0be561d89d48",
   "metadata": {},
   "source": [
    "## Time\n",
    "Finally, we can also include time-based features such as month, day of the month, day of the year, and year. Since we have removed time dependence, it is likely that none of these features will be considered relevant. However, because tree-based models are non-linear, it's possible that this information, combined with other features, could reveal patterns not already captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d20803-eb6d-4e06-9c82-78491518a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_month'] = df['time'].dt.day\n",
    "df['day_year'] = df['time'].dt.day_of_year\n",
    "df['month'] = df['time'].dt.month.astype('category')\n",
    "df['year'] =  df['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2420aa-d34f-416d-977a-010d88295835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 4394)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bc414-03fb-4140-853b-27bc4b51dc80",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We have 4,396 features however, within this pool, we have 12 that cannot be used. To select the best model for this task, we will perform cross-validation using all features with the simplest version of each model and measure their Mean Absolute Error (MAE). The model that performs best will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5791675a-265a-4e79-ab7c-6f52c51e9cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all_columns = set(df.columns)\n",
    "set_columns = set_all_columns - not_include\n",
    "len(set_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad1e69-a747-4ba8-9ff1-b4269fa37e75",
   "metadata": {},
   "source": [
    "Above we have the number of features used for the model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b3d487-edd5-4205-a3fc-c76071b5baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "X = df[list(set_columns)]\n",
    "y = df['ml_resid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87bf3911-b256-4d51-be1a-babd82f4f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb  = XGBRegressor(random_state=25, enable_categorical=True)\n",
    "model_lgbm = LGBMRegressor(random_state=25)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c00389b-2782-4bd4-a311-3eef60edfc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean MAE:  2.5637\n",
      "LightGBM Mean MAE: 2.4818\n"
     ]
    }
   ],
   "source": [
    "mae_xgb  = -cross_val_score(model_xgb,  X, y, cv=cv, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "mae_lgbm = -cross_val_score(model_lgbm, X, y, cv=cv, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "print(f\"XGBoost Mean MAE:  {mae_xgb.mean():.4f}\")\n",
    "print(f\"LightGBM Mean MAE: {mae_lgbm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62e362-aea7-4946-9376-ff87f5d3d7d7",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Based on the MAE results, we selected LightGBM due to its superior performance. Next, we proceed to feature selection. The chosen method is straightforward: we first train our model on all available data and remove features that show no importance.\n",
    "\n",
    "We then apply a Greedy Forward Selection method, a stepwise feature selection approach that iteratively builds a model by adding features one at a time based on their contribution to predictive performance. Starting with an empty set of features, the algorithm evaluates each candidate feature by temporarily including it in the model and measuring the improvement in a performance metric, such as mean absolute error (MAE). The feature that yields the largest improvement is permanently added to the model.\n",
    "\n",
    "This process repeats until adding new features no longer significantly improves performance or a predefined stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae53d7-5c6e-45f7-9675-c9da5aa3d595",
   "metadata": {},
   "source": [
    "## Gain Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf855e26-c007-463a-a344-5649ed8cf62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.455222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 717576\n",
      "[LightGBM] [Info] Number of data points in the train set: 1825, number of used features: 4384\n",
      "[LightGBM] [Info] Start training from score -0.049022\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(random_state=25).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8249f4f0-3eea-4f61-83aa-4f43a1263530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>tavg_430_diff</td>\n",
       "      <td>835.089394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>wspd_655_diff</td>\n",
       "      <td>778.790444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>wspd_560_diff</td>\n",
       "      <td>603.810895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>snow_402</td>\n",
       "      <td>596.010201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>pres_390</td>\n",
       "      <td>572.651686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2355</th>\n",
       "      <td>tavg_538_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>snow_640_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>tavg_620</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>prcp_624_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>tavg_569_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature        gain\n",
       "1261  tavg_430_diff  835.089394\n",
       "1701  wspd_655_diff  778.790444\n",
       "2809  wspd_560_diff  603.810895\n",
       "625        snow_402  596.010201\n",
       "487        pres_390  572.651686\n",
       "...             ...         ...\n",
       "2355  tavg_538_diff    0.000000\n",
       "2354  snow_640_diff    0.000000\n",
       "797        tavg_620    0.000000\n",
       "2383  prcp_624_diff    0.000000\n",
       "773   tavg_569_diff    0.000000\n",
       "\n",
       "[4384 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_gain = model.booster_.feature_importance(importance_type='gain')\n",
    "feature_names = model.booster_.feature_name()\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'gain': importance_gain\n",
    "}).sort_values(by='gain', ascending=False)\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770ff640-3057-4657-ab45-af4ce18dbf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1806.000000\n",
       "mean       52.722365\n",
       "std        74.959522\n",
       "min         0.731386\n",
       "25%         5.416520\n",
       "50%        23.570001\n",
       "75%        75.134867\n",
       "max       835.089394\n",
       "Name: gain, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp.loc[feat_imp['gain'] > 0, 'gain'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a8be6-9fe9-4f30-9b70-cd83e8c9410b",
   "metadata": {},
   "source": [
    "Even though we removed around 2,500 features that showed no gain, we still have too many features to perform greedy feature selection. Therefore, we will retain only the features that contribute more than 0.05% of the total gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3403017d-eb46-47c2-8cc0-4f0da4cf84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pool = feat_imp.loc[feat_imp['gain'] > feat_imp['gain'].sum()* 0.0005, 'feature'].tolist()\n",
    "len(feature_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd6217-f76e-413a-b619-41d0bdaa18f9",
   "metadata": {},
   "source": [
    "## Greedy Selection\n",
    "This method leverages the lightGBM ability to handle complex, nonlinear relationships and interactions among features, making it an effective way to identify a predictive subset of features while reducing dimensionality and potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bedd63b5-d07e-4431-9ef7-172c9296074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature(feature, current_features, X, y):\n",
    "    features_to_use = current_features + [feature]\n",
    "    cv = ShuffleSplit(n_splits=3, test_size=0.25, random_state=42)\n",
    "    maes = []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx][features_to_use], X.iloc[val_idx][features_to_use]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = LGBMRegressor(num_boost_round = 75, max_depth = 4, subsample = 0.6, verbose=-1, random_state=25)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        maes.append(mean_absolute_error(y_val, y_pred))\n",
    "        \n",
    "    return np.mean(maes), feature\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = feature_pool\n",
    "performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e43b7c52-1589-43b2-9998-c469ed09ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step_1: 100%|███████████████████████████████████████████████████████████████████████████████████| 627/627 [06:36<00:00,  1.58it/s]\n",
      "step_2 (MAE: 2.4025): 100%|████████████████████████████████████████████████████████████████████████████| 626/626 [06:16<00:00,  1.66it/s]\n",
      "step_3 (MAE: 2.3824): 100%|████████████████████████████████████████████████████████████████████████████| 625/625 [06:20<00:00,  1.64it/s]\n",
      "step_4 (MAE: 2.3719): 100%|████████████████████████████████████████████████████████████████████████████| 624/624 [06:26<00:00,  1.61it/s]\n",
      "step_5 (MAE: 2.3590): 100%|████████████████████████████████████████████████████████████████████████████| 623/623 [06:30<00:00,  1.60it/s]\n",
      "step_6 (MAE: 2.3392): 100%|█████████████████████████████████████████████████████████████████████| 622/622 [06:30<00:00,  1.59it/s]\n",
      "step_7 (MAE: 2.3220): 100%|████████████████████████████████████████████████████████████████████████████| 621/621 [06:45<00:00,  1.53it/s]\n",
      "step_8 (MAE: 2.3149): 100%|████████████████████████████████████████████████████████████████████████████| 620/620 [06:57<00:00,  1.48it/s]\n",
      "step_9 (MAE: 2.3114): 100%|████████████████████████████████████████████████████████████████████████████| 619/619 [06:50<00:00,  1.51it/s]\n",
      "step_10 (MAE: 2.3002): 100%|███████████████████████████████████████████████████████████████████████████| 618/618 [06:57<00:00,  1.48it/s]\n",
      "step_11 (MAE: 2.3016): 100%|████████████████████████████████████████████████████████████████████| 617/617 [06:51<00:00,  1.50it/s]\n",
      "step_12 (MAE: 2.2930): 100%|███████████████████████████████████████████████████████████████████████████| 616/616 [06:54<00:00,  1.48it/s]\n",
      "step_13 (MAE: 2.2953): 100%|███████████████████████████████████████████████████████████████████████████| 615/615 [07:18<00:00,  1.40it/s]\n",
      "step_14 (MAE: 2.2928): 100%|████████████████████████████████████████████████████████████████████| 614/614 [07:02<00:00,  1.45it/s]\n",
      "step_15 (MAE: 2.2891): 100%|████████████████████████████████████████████████████████████████████| 613/613 [07:00<00:00,  1.46it/s]\n",
      "step_16 (MAE: 2.2894): 100%|████████████████████████████████████████████████████████████████████| 612/612 [07:00<00:00,  1.46it/s]\n",
      "step_17 (MAE: 2.2835): 100%|████████████████████████████████████████████████████████████████████| 611/611 [07:00<00:00,  1.45it/s]\n",
      "step_18 (MAE: 2.2889): 100%|████████████████████████████████████████████████████████████████████| 610/610 [07:01<00:00,  1.45it/s]\n",
      "step_19 (MAE: 2.2827): 100%|████████████████████████████████████████████████████████████████████| 609/609 [06:38<00:00,  1.53it/s]\n",
      "step_20 (MAE: 2.2885): 100%|███████████████████████████████████████████████████████████████████████████| 608/608 [06:38<00:00,  1.53it/s]\n",
      "step_21 (MAE: 2.2803): 100%|███████████████████████████████████████████████████████████████████████████| 607/607 [06:33<00:00,  1.54it/s]\n",
      "step_22 (MAE: 2.2793): 100%|███████████████████████████████████████████████████████████████████████████| 606/606 [06:44<00:00,  1.50it/s]\n",
      "step_23 (MAE: 2.2766): 100%|████████████████████████████████████████████████████████████████████| 605/605 [07:18<00:00,  1.38it/s]\n",
      "step_24 (MAE: 2.2713): 100%|████████████████████████████████████████████████████████████████████| 604/604 [06:41<00:00,  1.50it/s]\n",
      "step_25 (MAE: 2.2624): 100%|████████████████████████████████████████████████████████████████████| 603/603 [06:42<00:00,  1.50it/s]\n",
      "step_26 (MAE: 2.2692): 100%|████████████████████████████████████████████████████████████████████| 602/602 [07:12<00:00,  1.39it/s]\n",
      "step_27 (MAE: 2.2629): 100%|████████████████████████████████████████████████████████████████████| 601/601 [06:41<00:00,  1.50it/s]\n",
      "step_28 (MAE: 2.2588): 100%|████████████████████████████████████████████████████████████████████| 600/600 [06:39<00:00,  1.50it/s]\n",
      "step_29 (MAE: 2.2583): 100%|███████████████████████████████████████████████████████████████████████████| 599/599 [07:20<00:00,  1.36it/s]\n",
      "step_30 (MAE: 2.2525): 100%|███████████████████████████████████████████████████████████████████████████| 598/598 [06:45<00:00,  1.47it/s]\n",
      "step_31 (MAE: 2.2574): 100%|████████████████████████████████████████████████████████████████████| 597/597 [06:44<00:00,  1.48it/s]\n",
      "step_32 (MAE: 2.2531): 100%|████████████████████████████████████████████████████████████████████| 596/596 [07:35<00:00,  1.31it/s]\n",
      "step_33 (MAE: 2.2544): 100%|████████████████████████████████████████████████████████████████████| 595/595 [06:56<00:00,  1.43it/s]\n",
      "step_34 (MAE: 2.2495): 100%|████████████████████████████████████████████████████████████████████| 594/594 [07:20<00:00,  1.35it/s]\n",
      "step_35 (MAE: 2.2602): 100%|████████████████████████████████████████████████████████████████████| 593/593 [07:32<00:00,  1.31it/s]\n",
      "step_36 (MAE: 2.2628): 100%|████████████████████████████████████████████████████████████████████| 592/592 [07:47<00:00,  1.27it/s]\n",
      "step_37 (MAE: 2.2583): 100%|████████████████████████████████████████████████████████████████████| 591/591 [07:00<00:00,  1.41it/s]\n",
      "step_38 (MAE: 2.2506): 100%|████████████████████████████████████████████████████████████████████| 590/590 [07:02<00:00,  1.40it/s]\n",
      "step_39 (MAE: 2.2357): 100%|████████████████████████████████████████████████████████████████████| 589/589 [06:58<00:00,  1.41it/s]\n",
      "step_40 (MAE: 2.2295): 100%|███████████████████████████████████████████████████████████████████████████| 588/588 [06:58<00:00,  1.41it/s]\n",
      "step_41 (MAE: 2.2363): 100%|███████████████████████████████████████████████████████████████████████████| 587/587 [06:43<00:00,  1.45it/s]\n",
      "step_42 (MAE: 2.2390): 100%|███████████████████████████████████████████████████████████████████████████| 586/586 [06:38<00:00,  1.47it/s]\n",
      "step_43 (MAE: 2.2368): 100%|███████████████████████████████████████████████████████████████████████████| 585/585 [06:45<00:00,  1.44it/s]\n",
      "step_44 (MAE: 2.2367): 100%|███████████████████████████████████████████████████████████████████████████| 584/584 [06:54<00:00,  1.41it/s]\n",
      "step_45 (MAE: 2.2309): 100%|███████████████████████████████████████████████████████████████████████████| 583/583 [06:46<00:00,  1.44it/s]\n",
      "step_46 (MAE: 2.2404): 100%|███████████████████████████████████████████████████████████████████████████| 582/582 [06:44<00:00,  1.44it/s]\n",
      "step_47 (MAE: 2.2453): 100%|███████████████████████████████████████████████████████████████████████████| 581/581 [07:03<00:00,  1.37it/s]\n",
      "step_48 (MAE: 2.2441): 100%|████████████████████████████████████████████████████████████████████| 580/580 [07:04<00:00,  1.37it/s]\n",
      "step_49 (MAE: 2.2526): 100%|████████████████████████████████████████████████████████████████████| 579/579 [07:28<00:00,  1.29it/s]\n",
      "step_50 (MAE: 2.2497): 100%|███████████████████████████████████████████████████████████████████████████| 578/578 [07:15<00:00,  1.33it/s]\n",
      "step_51 (MAE: 2.2504):   7%|████▌                                                                | 38/577 [00:13<00:32, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.547382 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 668884\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 4384\n",
      "[LightGBM] [Info] Start training from score -0.065494\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.419361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669114\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 4384\n",
      "[LightGBM] [Info] Start training from score -0.095867\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.424431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 669030\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 4384\n",
      "[LightGBM] [Info] Start training from score -0.048396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m description = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperformance_history[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstep_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Parallel evaluation using joblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_feature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m best_mae, best_feature = \u001b[38;5;28mmin\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Check improvement\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pessoal/ChicagoWeatherForecast/venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pessoal/ChicagoWeatherForecast/venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Pessoal/ChicagoWeatherForecast/venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for step in range(1, 50):\n",
    "    description = f'step_{step} (MAE: {performance_history[-1]:.4f})' if step > 1 else f'step_{step}'\n",
    "\n",
    "    # Parallel evaluation using joblib\n",
    "    results = Parallel(n_jobs=-2)(\n",
    "        delayed(evaluate_feature)(f, selected_features, X, y) for f in tqdm(remaining_features, desc=description, leave=True, position=0)\n",
    "    )\n",
    "    best_mae, best_feature = min(results, key=lambda x: x[0])\n",
    "\n",
    "    # Check improvement\n",
    "    if step > 10 and best_mae - performance_history[-1] > 0.01:\n",
    "        print(\"No improvement, stopping selection.\")\n",
    "        break\n",
    "\n",
    "    # Save results\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    performance_history.append(best_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c72dc9-7ce8-430c-83d9-3a03585c594d",
   "metadata": {},
   "source": [
    "The code was manually interrupted because the metric was no longer improving as more features were added. The change was too small to trigger the early-stopping condition, but the metrics shown in the progress bar indicated that the model had already reached its best performance at the 39th step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c22874-2d9e-46e0-842b-b05ea8543db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pres_703',\n",
       "  'tamp_404',\n",
       "  'prcp_702',\n",
       "  'tamp_709_diff',\n",
       "  'wspd_718',\n",
       "  'prcp_411',\n",
       "  'wspd_646',\n",
       "  'tamp_582',\n",
       "  'tamp_523_diff',\n",
       "  'tamp_372',\n",
       "  'prcp_382',\n",
       "  'tamp_469',\n",
       "  'prcp_703',\n",
       "  'tavg_378_diff',\n",
       "  'wspd_657',\n",
       "  'tavg_412',\n",
       "  'wspd_416',\n",
       "  'tavg_486_diff',\n",
       "  'wspd_524',\n",
       "  'tamp_397_diff',\n",
       "  'snow_370_diff',\n",
       "  'pres_389_diff',\n",
       "  'wspd_597',\n",
       "  'pres_440_diff',\n",
       "  'tavg_682_diff',\n",
       "  'snow_694',\n",
       "  'tavg_695_diff',\n",
       "  'tamp_532_diff',\n",
       "  'tamp_382',\n",
       "  'wspd_401_diff',\n",
       "  'tavg_728',\n",
       "  'tamp_691_diff',\n",
       "  'prcp_376',\n",
       "  'tamp_403',\n",
       "  'tamp_490',\n",
       "  'tamp_375',\n",
       "  'wspd_464_diff',\n",
       "  'tamp_720_diff',\n",
       "  'wspd_600_diff',\n",
       "  'pres_717_diff',\n",
       "  'tavg_427_diff',\n",
       "  'wspd_613',\n",
       "  'tavg_497_diff',\n",
       "  'tamp_721',\n",
       "  'prcp_460_diff',\n",
       "  'prcp_614_diff',\n",
       "  'prcp_386',\n",
       "  'prcp_443_diff',\n",
       "  'tamp_517',\n",
       "  'tamp_456_diff'],\n",
       " [np.float64(2.4025164527247753),\n",
       "  np.float64(2.382404352509843),\n",
       "  np.float64(2.371872236610991),\n",
       "  np.float64(2.3590130649809837),\n",
       "  np.float64(2.339218500035797),\n",
       "  np.float64(2.322031321545662),\n",
       "  np.float64(2.314899680426213),\n",
       "  np.float64(2.311399603366361),\n",
       "  np.float64(2.3001867114857357),\n",
       "  np.float64(2.3015569300112975),\n",
       "  np.float64(2.2929890934069252),\n",
       "  np.float64(2.2952701660296055),\n",
       "  np.float64(2.292758615177181),\n",
       "  np.float64(2.2890978599256884),\n",
       "  np.float64(2.2893647124930685),\n",
       "  np.float64(2.283534788400379),\n",
       "  np.float64(2.288853202311492),\n",
       "  np.float64(2.282734721697176),\n",
       "  np.float64(2.288500115501647),\n",
       "  np.float64(2.2803279657622926),\n",
       "  np.float64(2.279305541214456),\n",
       "  np.float64(2.276634517941441),\n",
       "  np.float64(2.2713070342191526),\n",
       "  np.float64(2.262353230146099),\n",
       "  np.float64(2.269184686234561),\n",
       "  np.float64(2.262894977635177),\n",
       "  np.float64(2.2587653872395013),\n",
       "  np.float64(2.2583353416953496),\n",
       "  np.float64(2.2524634428194403),\n",
       "  np.float64(2.2573566454755265),\n",
       "  np.float64(2.25309260743799),\n",
       "  np.float64(2.254369598932876),\n",
       "  np.float64(2.2495158475447288),\n",
       "  np.float64(2.2602393000638727),\n",
       "  np.float64(2.2627707718982357),\n",
       "  np.float64(2.2582531545726017),\n",
       "  np.float64(2.2506337373210212),\n",
       "  np.float64(2.2357338758354044),\n",
       "  np.float64(2.2295474808738347),\n",
       "  np.float64(2.236301965968003),\n",
       "  np.float64(2.2389840864947512),\n",
       "  np.float64(2.236768097519516),\n",
       "  np.float64(2.2367222536277036),\n",
       "  np.float64(2.230870525955146),\n",
       "  np.float64(2.2404223317631673),\n",
       "  np.float64(2.24533191289026),\n",
       "  np.float64(2.2441213824283572),\n",
       "  np.float64(2.252606548824982),\n",
       "  np.float64(2.2497457406767163),\n",
       "  np.float64(2.2504338617108886)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features, performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bae2c-8d2c-456a-a27f-34f09f7a2902",
   "metadata": {},
   "source": [
    "Using the method above, we managed to select 39 predictive features, significantly reducing the dimensionality. For comparison, these 39 features, combined with simpler hyperparameters, achieved an MAE that was 0,25 points lower than the model trained with all 4,000 features during \"Model Selection\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f98027d1-8f32-4470-afda-f1284fc1e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 38)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[selected_features[:performance_history.index(min(performance_history))]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a997903-fcbb-4545-8844-5d028ec7f6cc",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "After selecting the predictive features, we move on to tuning the model. The strategy is simple: we use Bayesian Optimization, which learns from the results of past trials to make informed decisions about which hyperparameters to evaluate next, to identify promising regions of the search space. Within those regions, we then apply a GridSearch to determine the best hyperparameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733ad0c-1bfb-4982-b459-80b8c121e2a3",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "Optuna is a hyperparameter optimization framework that incorporates Bayesian optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f79965d0-2e04-489b-93b3-436a5d9ca2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_values = X.values\n",
    "y_values = y.values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1500),\n",
    "\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-5, 1.0),\n",
    "\n",
    "        \"min_gain_to_split\": 0.0,\n",
    "\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 2.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 2.0),\n",
    "\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": 1,\n",
    "    }\n",
    "\n",
    "    maes = []\n",
    "    best_iters = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_values):\n",
    "        X_train, X_valid = X_values[train_idx], X_values[valid_idx]\n",
    "        y_train, y_valid = y_values[train_idx], y_values[valid_idx]\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            callbacks=[\n",
    "                early_stopping(200, verbose=False),  \n",
    "                log_evaluation(period=0)            \n",
    "            ]\n",
    "        )\n",
    "        best_iters.append(model.best_iteration_)\n",
    "        pred = model.predict(X_valid)\n",
    "        maes.append(mean_absolute_error(y_valid, pred))\n",
    "        \n",
    "    trial.set_user_attr(\"mean_best_iteration\", np.mean(best_iters))\n",
    "    \n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f38c0b7c-7c49-426c-9f09-a8d6cf2e75fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Search: 100%|████████████████████████████████████████████████████████████████████| 100/100 [08:27<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Optuna params:\n",
      "{'num_leaves': 188, 'max_depth': 3, 'learning_rate': 0.0476510398940014, 'n_estimators': 1378, 'min_child_samples': 17, 'min_child_weight': 0.8738048497952383, 'lambda_l1': 0.17405239654238058, 'lambda_l2': 0.48525397028025363, 'feature_fraction': 0.8046112243391896, 'bagging_fraction': 0.790649561847512}\n",
      "133.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "with tqdm(total=n_trials, desc=\"Hyperparameter Search\", position=0) as pbar:\n",
    "    for _ in range(n_trials):\n",
    "        study.optimize(objective, n_trials=1, catch=(Exception,))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"Best Optuna params:\")\n",
    "print(study.best_params)\n",
    "print(\"Average n_estimator:\", study.best_trial.user_attrs[\"mean_best_iteration\"])\n",
    "\n",
    "best = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cf26e-1fd0-43b1-acb5-a26f17b1e409",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "With the promising search areas defined, we proceed to a GridSearch to identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9db2a0a-25c5-4010-a145-1c4f1a675f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "04435b21-477e-460f-b0b6-060e7ce3caec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.03335572792580098,\n",
       "  0.0476510398940014,\n",
       "  0.06194635186220182],\n",
       " 'num_leaves': [172, 188, 204],\n",
       " 'min_child_samples': [12, 17, 22],\n",
       " 'feature_fraction': [0.7241501019052707,\n",
       "  0.8046112243391896,\n",
       "  0.8850723467731086],\n",
       " 'bagging_fraction': [0.7115846056627608,\n",
       "  0.790649561847512,\n",
       "  0.8697145180322633],\n",
       " 'lambda_l1': [0.1218366775796664, 0.17405239654238058, 0.22626811550509476],\n",
       " 'lambda_l2': [0.33967777919617753, 0.48525397028025363, 0.6308301613643298],\n",
       " 'n_estimators': [106, 133, 159]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {\n",
    "    \"learning_rate\": [max(1e-5, best[\"learning_rate\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"num_leaves\": sorted(list({max(16, best[\"num_leaves\"] + d) for d in [-16, 0, 16]})),\n",
    "    \"min_child_samples\": sorted(list({max(2, best[\"min_child_samples\"] + d) for d in [-5, 0, 5]})),\n",
    "    \"feature_fraction\": sorted(list({min(1.0, best[\"feature_fraction\"] * f) for f in [0.9, 1.0, 1.1]})),\n",
    "    \"bagging_fraction\": sorted(list({min(1.0, best[\"bagging_fraction\"] * f) for f in [0.9, 1.0, 1.1]})),\n",
    "    \"lambda_l1\": [max(0, best[\"lambda_l1\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"lambda_l2\": [max(0, best[\"lambda_l2\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"n_estimators\": [int(study.best_trial.user_attrs[\"mean_best_iteration\"] * n) for n in [0.8, 1.0, 1.2]]\n",
    "}\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f42060cb-ba76-4d4f-a56c-e33ad7c3843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6561 candidates, totalling 32805 fits\n",
      "Grid best params: {'bagging_fraction': 0.7115846056627608, 'feature_fraction': 0.8046112243391896, 'lambda_l1': 0.22626811550509476, 'lambda_l2': 0.6308301613643298, 'learning_rate': 0.03335572792580098, 'min_child_samples': 22, 'n_estimators': 106, 'num_leaves': 172}\n",
      "Grid best MAE: 2.347952880634722\n"
     ]
    }
   ],
   "source": [
    "base_model = LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    random_state=25,\n",
    "    verbosity=-1,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=grid,\n",
    "    scoring=mae_scorer,\n",
    "    cv=kf,\n",
    "    n_jobs=-2,\n",
    "    verbose=1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Grid best params:\", grid_search.best_params_)\n",
    "print(\"Grid best MAE:\", -grid_search.best_score_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60bd64-6337-4ab5-bdde-82226911bdfa",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "Now that we have identified the best feature set and hyperparameters for our model, we can finalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "709519c2-cf73-4278-8df7-83e97cc07638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(\n",
    "    **grid_search.best_params_,\n",
    "    objective=\"regression\",\n",
    "    random_state=25,\n",
    "    n_jobs=-2,\n",
    "    verbosity=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "574ef648-1101-47d5-a3a3-bf0243552a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM All MAEs: [2.2375594  2.24416481 2.56208358 2.19960629 2.30541475 2.62504182\n",
      " 2.35684063 2.28164927 2.31679155 2.4487121 ]\n",
      "LightGBM Mean MAE: 2.3578\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=25)\n",
    "final_lgbm = -cross_val_score(model, X, y, cv=kf, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "print(f\"LightGBM All MAEs: {final_lgbm}\")\n",
    "print(f\"LightGBM Mean MAE: {final_lgbm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9f7eefc8-d154-4bb7-98d4-b997aeaf0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "light = model.fit(X, y)\n",
    "pickle.dump(light, open('lightgbm_model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
