{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c8a30a-5014-42f8-b0c1-193817c2ccff",
   "metadata": {},
   "source": [
    "# Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6727877-54ff-4b33-8daa-dace7e131f61",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "In the previous notebook, ts_model.ipynb, we managed to remove all time dependency, seasonality and autocorrelation, from our target feature, allowing us to transition to a classic machine learning approach, specifically, a tree-based model.\n",
    "\n",
    "In this notebook, we performed feature engineering, model selection, feature selection, and finally model tuning. The candidate models for this problem are LightGBM and XGBoost, due to their typically high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb6c70-f7a8-4f8b-9aba-e17e62fc0f0f",
   "metadata": {},
   "source": [
    "**Data Source**\n",
    "The data used in this notebook was extracted from the notebook *model/ts_model.ipynb*\n",
    "\n",
    "- **Data:** 06/12/2025\n",
    "- **Localização:** ../data/wrangle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc8397c-3c95-4a9d-98c4-75e679c7ab53",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7a357-7730-4a42-9510-0f21ecab6526",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19eeb0ac-5ec3-4430-a65d-1aa8c692d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro.moura/Pessoal/ChicagoWeatherForecast/venv/lib/python3.12/site-packages/tqdm_joblib/__init__.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "## Base\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler #\n",
    "from sklearn.model_selection import KFold, ShuffleSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"lightgbm.engine\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"lightgbm.basic\").setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1083c3c0-c686-4571-b78d-93c1cde9e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções criadas\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(1, Path.cwd().parents[1].as_posix())\n",
    "\n",
    "from src.ts_utils import *\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543217c0-0486-4e8b-8998-4696c4e01d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=['#003366'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0640dd-4a68-4671-b1df-b8d860e47003",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Our target variable contains 6 years of data, totaling 2,190 rows, due to the seasonal division applied in the previous notebook. This amount of information is typically small for training tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab5fc23-5909-4d83-8082-b2cdff78324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2555 entries, 0 to 2554\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   time      2555 non-null   datetime64[ns]\n",
      " 1   tavg      2555 non-null   float64       \n",
      " 2   prcp      2555 non-null   float64       \n",
      " 3   snow      2555 non-null   float64       \n",
      " 4   wspd      2555 non-null   float64       \n",
      " 5   pres      2555 non-null   float64       \n",
      " 6   tamp      2555 non-null   float64       \n",
      " 7   sin_1     2555 non-null   float64       \n",
      " 8   cos_1     2555 non-null   float64       \n",
      " 9   ml_resid  2555 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9)\n",
      "memory usage: 199.7 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(os.path.join(DATA_PATH_WRANGLE, 'weather_linear_resid.parquet'))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea752489-5b37-40b4-9f66-41751c5007a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    2555.000000\n",
       " mean        0.002710\n",
       " std         3.179228\n",
       " min       -15.814890\n",
       " 25%        -1.684089\n",
       " 50%         0.196558\n",
       " 75%         1.828278\n",
       " max        11.023475\n",
       " Name: ml_resid, dtype: float64,\n",
       " np.float64(2.3779238494591897))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ml_resid'].describe(), abs(df['ml_resid']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117b479-9a01-48ba-9923-05354e29073a",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "In this section, we create all the features our models might use. The plan is to generate as many features as possible to expose all the information contained in the data and then later decide which ones should remain in the final model. In other words, we are not concerned at this stage with whether a feature is useful, this will be determined later in the notebook.\n",
    "\n",
    "The only rule is that the feature must be available at the time of forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12caa6d5-2936-4259-b2d0-802193af4647",
   "metadata": {},
   "source": [
    "## Lagging\n",
    "\n",
    "Given this rule, we begin by lagging all relevant features to ensure they are available at forecast time. We applied lags from 365 to 730 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c05baf-95e0-4bff-9cb1-07765bf9b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['tavg', 'prcp', 'snow', 'wspd', 'pres', 'tamp']\n",
    "for c in list_columns:\n",
    "    for lag in range(365, 2*365):\n",
    "        df[f'{c}_{lag}'] = df[c].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e6c3a85-b8e7-4d0e-adbe-8ce047189b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1826, 2200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05c872-7412-4da0-9ecd-75d9eb9b0a8e",
   "metadata": {},
   "source": [
    "So far, our table has 2.202 columns and 1.826 rows, about 5 years of data. After applying the lags, we will also include the differences, which capture the day-to-day changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7b478-f15c-4f2f-8c29-2f85a50d976f",
   "metadata": {},
   "source": [
    "## Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee97d9b1-0421-40f1-82b7-61eacba39913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2190"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_include = {'time', 'tavg', 'prcp', 'snow', 'wspd', 'pres', 'tamp', 'sin_1', 'cos_1', 'seasonal_resid', 'ml_resid', 'seasonal_pattern'}\n",
    "set_all_columns = set(df.columns)\n",
    "set_columns = set_all_columns - not_include\n",
    "\n",
    "len(set_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d3a655-2743-469b-9113-a67c5d95b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in set_columns:\n",
    "    df[f'{c}_diff'] = df[c].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8767e6a3-1fde-42e7-84a4-16d2f24909b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 4390)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd091e5a-0fa9-40d6-8643-0be561d89d48",
   "metadata": {},
   "source": [
    "## Time\n",
    "Finally, we can also include time-based features such as month, day of the month, day of the year, and year. Since we have removed time dependence, it is likely that none of these features will be considered relevant. However, because tree-based models are non-linear, it's possible that this information, combined with other features, could reveal patterns not already captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d20803-eb6d-4e06-9c82-78491518a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_month'] = df['time'].dt.day\n",
    "df['day_year'] = df['time'].dt.day_of_year\n",
    "df['month'] = df['time'].dt.month.astype('category')\n",
    "df['year'] =  df['time'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2420aa-d34f-416d-977a-010d88295835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 4394)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bc414-03fb-4140-853b-27bc4b51dc80",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We have 4,396 features however, within this pool, we have 12 that cannot be used. To select the best model for this task, we will perform cross-validation using all features with the simplest version of each model and measure their Mean Absolute Error (MAE). The model that performs best will be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5791675a-265a-4e79-ab7c-6f52c51e9cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_all_columns = set(df.columns)\n",
    "set_columns = set_all_columns - not_include\n",
    "len(set_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ad1e69-a747-4ba8-9ff1-b4269fa37e75",
   "metadata": {},
   "source": [
    "Above we have the number of features used for the model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3b3d487-edd5-4205-a3fc-c76071b5baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "X = df[list(set_columns)]\n",
    "y = df['ml_resid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87bf3911-b256-4d51-be1a-babd82f4f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb  = XGBRegressor(random_state=25, enable_categorical=True)\n",
    "model_lgbm = LGBMRegressor(random_state=25)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c00389b-2782-4bd4-a311-3eef60edfc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean MAE:  2.5654\n",
      "LightGBM Mean MAE: 2.4818\n"
     ]
    }
   ],
   "source": [
    "mae_xgb  = -cross_val_score(model_xgb,  X, y, cv=cv, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "mae_lgbm = -cross_val_score(model_lgbm, X, y, cv=cv, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "print(f\"XGBoost Mean MAE:  {mae_xgb.mean():.4f}\")\n",
    "print(f\"LightGBM Mean MAE: {mae_lgbm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62e362-aea7-4946-9376-ff87f5d3d7d7",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Based on the MAE results, we selected LightGBM due to its superior performance. Next, we proceed to feature selection. The chosen method is straightforward: we first train our model on all available data and remove features that show no importance.\n",
    "\n",
    "We then apply a Greedy Forward Selection method, a stepwise feature selection approach that iteratively builds a model by adding features one at a time based on their contribution to predictive performance. Starting with an empty set of features, the algorithm evaluates each candidate feature by temporarily including it in the model and measuring the improvement in a performance metric, such as mean absolute error (MAE). The feature that yields the largest improvement is permanently added to the model.\n",
    "\n",
    "This process repeats until adding new features no longer significantly improves performance or a predefined stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae53d7-5c6e-45f7-9675-c9da5aa3d595",
   "metadata": {},
   "source": [
    "## Gain Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf855e26-c007-463a-a344-5649ed8cf62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.161860 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 717576\n",
      "[LightGBM] [Info] Number of data points in the train set: 1825, number of used features: 4384\n",
      "[LightGBM] [Info] Start training from score -0.049022\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(random_state=25).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8249f4f0-3eea-4f61-83aa-4f43a1263530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tavg_430_diff</td>\n",
       "      <td>835.089394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>wspd_655_diff</td>\n",
       "      <td>778.790444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>wspd_560_diff</td>\n",
       "      <td>603.810895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>snow_402</td>\n",
       "      <td>596.010201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>pres_390</td>\n",
       "      <td>572.651686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>prcp_669</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>prcp_691</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>prcp_654_diff</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>prcp_383</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>snow_623</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4384 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature        gain\n",
       "0     tavg_430_diff  835.089394\n",
       "761   wspd_655_diff  778.790444\n",
       "3034  wspd_560_diff  603.810895\n",
       "2704       snow_402  596.010201\n",
       "3180       pres_390  572.651686\n",
       "...             ...         ...\n",
       "1291       prcp_669    0.000000\n",
       "1292       prcp_691    0.000000\n",
       "1294  prcp_654_diff    0.000000\n",
       "1295       prcp_383    0.000000\n",
       "2537       snow_623    0.000000\n",
       "\n",
       "[4384 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_gain = model.booster_.feature_importance(importance_type='gain')\n",
    "feature_names = model.booster_.feature_name()\n",
    "\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'gain': importance_gain\n",
    "}).sort_values(by='gain', ascending=False)\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "770ff640-3057-4657-ab45-af4ce18dbf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1806.000000\n",
       "mean       52.722365\n",
       "std        74.959522\n",
       "min         0.731386\n",
       "25%         5.416520\n",
       "50%        23.570001\n",
       "75%        75.134867\n",
       "max       835.089394\n",
       "Name: gain, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_imp.loc[feat_imp['gain'] > 0, 'gain'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a8be6-9fe9-4f30-9b70-cd83e8c9410b",
   "metadata": {},
   "source": [
    "Even though we removed around 2,500 features that showed no gain, we still have too many features to perform greedy feature selection. Therefore, we will retain only the features that contribute more than 0.05% of the total gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3403017d-eb46-47c2-8cc0-4f0da4cf84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_pool = feat_imp.loc[feat_imp['gain'] > feat_imp['gain'].sum()* 0.0005, 'feature'].tolist()\n",
    "len(feature_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd6217-f76e-413a-b619-41d0bdaa18f9",
   "metadata": {},
   "source": [
    "## Greedy Selection\n",
    "This method leverages the lightGBM ability to handle complex, nonlinear relationships and interactions among features, making it an effective way to identify a predictive subset of features while reducing dimensionality and potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bedd63b5-d07e-4431-9ef7-172c9296074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature(feature, current_features, X, y):\n",
    "    features_to_use = current_features + [feature]\n",
    "    cv = ShuffleSplit(n_splits=3, test_size=0.25, random_state=42)\n",
    "    maes = []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx][features_to_use], X.iloc[val_idx][features_to_use]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        model = LGBMRegressor(num_boost_round = 75, max_depth = 4, subsample = 0.6, verbose=-1, random_state=25)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        maes.append(mean_absolute_error(y_val, y_pred))\n",
    "        \n",
    "    return np.mean(maes), feature\n",
    "\n",
    "selected_features = []\n",
    "remaining_features = feature_pool\n",
    "performance_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e43b7c52-1589-43b2-9998-c469ed09ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "step_1: 100%|█████████████████████████████████████████████████████████████████████| 627/627 [06:41<00:00,  1.56it/s]\n",
      "step_2 (MAE: 2.4025): 100%|█████████████████████████████████████████████████████████████| 626/626 [06:19<00:00,  1.65it/s]\n",
      "step_3 (MAE: 2.3824): 100%|█████████████████████████████████████████████████████████████| 625/625 [06:22<00:00,  1.64it/s]\n",
      "step_4 (MAE: 2.3719): 100%|█████████████████████████████████████████████████████████████| 624/624 [06:23<00:00,  1.63it/s]\n",
      "step_5 (MAE: 2.3590): 100%|█████████████████████████████████████████████████████████████| 623/623 [06:35<00:00,  1.58it/s]\n",
      "step_6 (MAE: 2.3392): 100%|█████████████████████████████████████████████████████████████| 622/622 [06:28<00:00,  1.60it/s]\n",
      "step_7 (MAE: 2.3220): 100%|█████████████████████████████████████████████████████████████| 621/621 [06:48<00:00,  1.52it/s]\n",
      "step_8 (MAE: 2.3149): 100%|█████████████████████████████████████████████████████████████| 620/620 [06:52<00:00,  1.50it/s]\n",
      "step_9 (MAE: 2.3114): 100%|█████████████████████████████████████████████████████████████| 619/619 [06:59<00:00,  1.48it/s]\n",
      "step_10 (MAE: 2.3002): 100%|████████████████████████████████████████████████████████████| 618/618 [06:52<00:00,  1.50it/s]\n",
      "step_11 (MAE: 2.3016): 100%|████████████████████████████████████████████████████████████| 617/617 [06:54<00:00,  1.49it/s]\n",
      "step_12 (MAE: 2.2930): 100%|████████████████████████████████████████████████████████████| 616/616 [06:59<00:00,  1.47it/s]\n",
      "step_13 (MAE: 2.2953): 100%|████████████████████████████████████████████████████████████| 615/615 [06:56<00:00,  1.48it/s]\n",
      "step_14 (MAE: 2.2928): 100%|████████████████████████████████████████████████████████████| 614/614 [06:59<00:00,  1.46it/s]\n",
      "step_15 (MAE: 2.2891): 100%|████████████████████████████████████████████████████████████| 613/613 [06:55<00:00,  1.47it/s]\n",
      "step_16 (MAE: 2.2894): 100%|████████████████████████████████████████████████████████████| 612/612 [06:57<00:00,  1.47it/s]\n",
      "step_17 (MAE: 2.2835): 100%|████████████████████████████████████████████████████████████| 611/611 [06:59<00:00,  1.46it/s]\n",
      "step_18 (MAE: 2.2889): 100%|████████████████████████████████████████████████████████████| 610/610 [07:05<00:00,  1.43it/s]\n",
      "step_19 (MAE: 2.2827): 100%|████████████████████████████████████████████████████████████| 609/609 [07:06<00:00,  1.43it/s]\n",
      "step_20 (MAE: 2.2885): 100%|████████████████████████████████████████████████████████████| 608/608 [07:03<00:00,  1.44it/s]\n",
      "step_21 (MAE: 2.2803): 100%|████████████████████████████████████████████████████████████| 607/607 [06:56<00:00,  1.46it/s]\n",
      "step_22 (MAE: 2.2793): 100%|████████████████████████████████████████████████████████████| 606/606 [06:48<00:00,  1.48it/s]\n",
      "step_23 (MAE: 2.2766): 100%|████████████████████████████████████████████████████████████| 605/605 [06:48<00:00,  1.48it/s]\n",
      "step_24 (MAE: 2.2713): 100%|████████████████████████████████████████████████████████████| 604/604 [06:51<00:00,  1.47it/s]\n",
      "step_25 (MAE: 2.2624): 100%|████████████████████████████████████████████████████████████| 603/603 [07:12<00:00,  1.39it/s]\n",
      "step_26 (MAE: 2.2692): 100%|████████████████████████████████████████████████████████████| 602/602 [06:47<00:00,  1.48it/s]\n",
      "step_27 (MAE: 2.2629): 100%|████████████████████████████████████████████████████████████| 601/601 [06:47<00:00,  1.47it/s]\n",
      "step_28 (MAE: 2.2588): 100%|████████████████████████████████████████████████████████████| 600/600 [06:52<00:00,  1.46it/s]\n",
      "step_29 (MAE: 2.2583): 100%|████████████████████████████████████████████████████████████| 599/599 [06:47<00:00,  1.47it/s]\n",
      "step_30 (MAE: 2.2525): 100%|████████████████████████████████████████████████████████████| 598/598 [06:58<00:00,  1.43it/s]\n",
      "step_31 (MAE: 2.2574): 100%|████████████████████████████████████████████████████████████| 597/597 [06:50<00:00,  1.46it/s]\n",
      "step_32 (MAE: 2.2531): 100%|████████████████████████████████████████████████████████████| 596/596 [06:50<00:00,  1.45it/s]\n",
      "step_33 (MAE: 2.2544): 100%|████████████████████████████████████████████████████████████| 595/595 [06:56<00:00,  1.43it/s]\n",
      "step_34 (MAE: 2.2495): 100%|████████████████████████████████████████████████████████████| 594/594 [06:50<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement, stopping selection.\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, 50):\n",
    "    description = f'step_{step} (MAE: {performance_history[-1]:.4f})' if step > 1 else f'step_{step}'\n",
    "\n",
    "    # Parallel evaluation using joblib\n",
    "    results = Parallel(n_jobs=-2)(\n",
    "        delayed(evaluate_feature)(f, selected_features, X, y) for f in tqdm(remaining_features, desc=description, leave=True, position=0)\n",
    "    )\n",
    "    best_mae, best_feature = min(results, key=lambda x: x[0])\n",
    "\n",
    "    # Check improvement\n",
    "    if step > 10 and best_mae - performance_history[-1] > 0.01:\n",
    "        print(\"No improvement, stopping selection.\")\n",
    "        break\n",
    "\n",
    "    # Save results\n",
    "    selected_features.append(best_feature)\n",
    "    remaining_features.remove(best_feature)\n",
    "    performance_history.append(best_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa481298-848d-40a3-b206-db572a758cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pres_703',\n",
       "  'tamp_404',\n",
       "  'prcp_702',\n",
       "  'tamp_709_diff',\n",
       "  'wspd_718',\n",
       "  'prcp_411',\n",
       "  'wspd_646',\n",
       "  'tamp_582',\n",
       "  'tamp_523_diff',\n",
       "  'tamp_372',\n",
       "  'prcp_382',\n",
       "  'tamp_469',\n",
       "  'prcp_703',\n",
       "  'tavg_378_diff',\n",
       "  'wspd_657',\n",
       "  'tavg_412',\n",
       "  'wspd_416',\n",
       "  'tavg_486_diff',\n",
       "  'wspd_524',\n",
       "  'tamp_397_diff',\n",
       "  'snow_370_diff',\n",
       "  'pres_389_diff',\n",
       "  'wspd_597',\n",
       "  'pres_440_diff',\n",
       "  'tavg_682_diff',\n",
       "  'snow_694',\n",
       "  'tavg_695_diff',\n",
       "  'tamp_532_diff',\n",
       "  'tamp_382',\n",
       "  'wspd_401_diff',\n",
       "  'tavg_728',\n",
       "  'tamp_691_diff',\n",
       "  'prcp_376'],\n",
       " [np.float64(2.4025164527247753),\n",
       "  np.float64(2.382404352509843),\n",
       "  np.float64(2.371872236610991),\n",
       "  np.float64(2.3590130649809837),\n",
       "  np.float64(2.339218500035797),\n",
       "  np.float64(2.322031321545662),\n",
       "  np.float64(2.314899680426213),\n",
       "  np.float64(2.311399603366361),\n",
       "  np.float64(2.3001867114857357),\n",
       "  np.float64(2.3015569300112975),\n",
       "  np.float64(2.2929890934069252),\n",
       "  np.float64(2.2952701660296055),\n",
       "  np.float64(2.292758615177181),\n",
       "  np.float64(2.2890978599256884),\n",
       "  np.float64(2.2893647124930685),\n",
       "  np.float64(2.283534788400379),\n",
       "  np.float64(2.288853202311492),\n",
       "  np.float64(2.282734721697176),\n",
       "  np.float64(2.288500115501647),\n",
       "  np.float64(2.2803279657622926),\n",
       "  np.float64(2.279305541214456),\n",
       "  np.float64(2.276634517941441),\n",
       "  np.float64(2.2713070342191526),\n",
       "  np.float64(2.262353230146099),\n",
       "  np.float64(2.269184686234561),\n",
       "  np.float64(2.262894977635177),\n",
       "  np.float64(2.2587653872395013),\n",
       "  np.float64(2.2583353416953496),\n",
       "  np.float64(2.2524634428194403),\n",
       "  np.float64(2.2573566454755265),\n",
       "  np.float64(2.25309260743799),\n",
       "  np.float64(2.254369598932876),\n",
       "  np.float64(2.2495158475447288)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features, performance_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bae2c-8d2c-456a-a27f-34f09f7a2902",
   "metadata": {},
   "source": [
    "Using the method above, we managed to select 34 predictive features, significantly reducing the dimensionality. For comparison, these 34 features, combined with simpler hyperparameters, achieved an MAE that was 0,25 points lower than the model trained with all 4,000 features during \"Model Selection\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f98027d1-8f32-4470-afda-f1284fc1e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1825, 32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[selected_features[:performance_history.index(min(performance_history))]]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a997903-fcbb-4545-8844-5d028ec7f6cc",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "After selecting the predictive features, we move on to tuning the model. The strategy is simple: we use Bayesian Optimization, which learns from the results of past trials to make informed decisions about which hyperparameters to evaluate next, to identify promising regions of the search space. Within those regions, we then apply a GridSearch to determine the best hyperparameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1733ad0c-1bfb-4982-b459-80b8c121e2a3",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n",
    "Optuna is a hyperparameter optimization framework that incorporates Bayesian optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79965d0-2e04-489b-93b3-436a5d9ca2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_values = X.values\n",
    "y_values = y.values\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 5, 50),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 800),\n",
    "\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 50),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-5, 1.0),\n",
    "\n",
    "        \"min_gain_to_split\": 0.0,\n",
    "\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 2.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 2.0),\n",
    "\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": 1,\n",
    "    }\n",
    "\n",
    "    maes = []\n",
    "    for train_idx, valid_idx in kf.split(X_values):\n",
    "        X_train, X_valid = X_values[train_idx], X_values[valid_idx]\n",
    "        y_train, y_valid = y_values[train_idx], y_values[valid_idx]\n",
    "\n",
    "        model = LGBMRegressor(**params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            callbacks=[\n",
    "                early_stopping(200, verbose=False),  \n",
    "                log_evaluation(period=0)            \n",
    "            ]\n",
    "        )\n",
    "        pred = model.predict(X_valid)\n",
    "        maes.append(mean_absolute_error(y_valid, pred))\n",
    "            \n",
    "    return np.mean(maes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f38c0b7c-7c49-426c-9f09-a8d6cf2e75fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Search: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:59<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Optuna params:\n",
      "{'num_leaves': 6, 'max_depth': 11, 'learning_rate': 0.0795599077344849, 'n_estimators': 280, 'min_child_samples': 16, 'min_child_weight': 0.5675840268741394, 'lambda_l1': 1.0533880401272688, 'lambda_l2': 1.5361414878826434, 'feature_fraction': 0.712067260102739, 'bagging_fraction': 0.8041159972132971}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "sampler = TPESampler(seed=25) \n",
    "study = optuna.create_study(sampler=sampler, direction=\"minimize\")\n",
    "\n",
    "with tqdm(total=n_trials, desc=\"Hyperparameter Search\", position=0) as pbar:\n",
    "    for _ in range(n_trials):\n",
    "        study.optimize(objective, n_trials=1, catch=(Exception,))\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"Best Optuna params:\")\n",
    "print(study.best_params)\n",
    "\n",
    "best = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cf26e-1fd0-43b1-acb5-a26f17b1e409",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "With the promising search areas defined, we proceed to a GridSearch to identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6139d36c-89e8-4326-94dc-83d572e1918e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': [0.05569193541413943,\n",
       "  0.0795599077344849,\n",
       "  0.10342788005483038],\n",
       " 'num_leaves': [2, 6, 10],\n",
       " 'min_child_samples': [11, 16, 21],\n",
       " 'feature_fraction': [0.6408605340924651,\n",
       "  0.712067260102739,\n",
       "  0.783273986113013],\n",
       " 'bagging_fraction': [0.7237043974919674,\n",
       "  0.8041159972132971,\n",
       "  0.8845275969346269],\n",
       " 'lambda_l1': [0.7373716280890882, 1.0533880401272688, 1.3694044521654496],\n",
       " 'lambda_l2': [1.0752990415178503, 1.5361414878826434, 1.9969839342474365],\n",
       " 'n_estimators': [224, 280, 336]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {\n",
    "    \"learning_rate\": [max(1e-5, best[\"learning_rate\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"num_leaves\": sorted(list({max(2, best[\"num_leaves\"] + d) for d in [-4, 0, 4]})),\n",
    "    \"min_child_samples\": sorted(list({max(2, best[\"min_child_samples\"] + d) for d in [-5, 0, 5]})),\n",
    "    \"feature_fraction\": sorted(list({min(1.0, best[\"feature_fraction\"] * f) for f in [0.9, 1.0, 1.1]})),\n",
    "    \"bagging_fraction\": sorted(list({min(1.0, best[\"bagging_fraction\"] * f) for f in [0.9, 1.0, 1.1]})),\n",
    "    \"lambda_l1\": [max(0, best[\"lambda_l1\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"lambda_l2\": [max(0, best[\"lambda_l2\"] * f) for f in [0.7, 1.0, 1.3]],\n",
    "    \"n_estimators\": [int(best[\"n_estimators\"] * n) for n in [0.8, 1.0, 1.2]]\n",
    "}\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f42060cb-ba76-4d4f-a56c-e33ad7c3843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid best params: {'bagging_fraction': 0.7237043974919674, 'feature_fraction': 0.783273986113013, 'lambda_l1': 1.0533880401272688, 'lambda_l2': 1.0752990415178503, 'learning_rate': 0.0795599077344849, 'min_child_samples': 16, 'n_estimators': 224, 'num_leaves': 2}\n",
      "Grid best MAE: 2.340156538817607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "base_model = LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    random_state=25,\n",
    "    verbosity=-1,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=grid,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    cv=kf,\n",
    "    n_jobs=-2,\n",
    "    verbose=0,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "total = 6561 * 5\n",
    "with tqdm_joblib(total=total, leave=False, position=0) as progress_bar:\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "print(\"Grid best params:\", grid_search.best_params_)\n",
    "print(\"Grid best MAE:\", -grid_search.best_score_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60bd64-6337-4ab5-bdde-82226911bdfa",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "Now that we have identified the best feature set and hyperparameters for our model, we can finalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "709519c2-cf73-4278-8df7-83e97cc07638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(\n",
    "    **grid_search.best_params_,\n",
    "    objective=\"regression\",\n",
    "    random_state=25,\n",
    "    n_jobs=-2,\n",
    "    verbosity=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "574ef648-1101-47d5-a3a3-bf0243552a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM All MAEs: [2.19056891 2.20933076 2.5819054  2.22072179 2.24458673 2.59071715\n",
      " 2.36637214 2.32592235 2.31028884 2.3984267 ]\n",
      "LightGBM Mean MAE: 2.3439\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=25)\n",
    "final_lgbm = -cross_val_score(model, X, y, cv=kf, n_jobs=-2, scoring=\"neg_mean_absolute_error\")\n",
    "print(f\"LightGBM All MAEs: {final_lgbm}\")\n",
    "print(f\"LightGBM Mean MAE: {final_lgbm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f7eefc8-d154-4bb7-98d4-b997aeaf0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "light = model.fit(X, y)\n",
    "pickle.dump(light, open('lightgbm_model.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
